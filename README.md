
<!-- <div align= "center">
    <h1> Official repo for Sparse-vDiT</h1>

</div> -->

<h3 align="center"><strong>Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers</strong></h3>

<div align="center">
<a href='https://arxiv.org/abs/2506.****'><img src='https://img.shields.io/badge/arXiv-2506.****-b31b1b.svg'></a> &nbsp;&nbsp;&nbsp;&nbsp;
</div>

<p align="center">
    <img src="assets/pipeline.png" alt="Demo GIF" width="720px" />
</p>

## ðŸ¥³ What's New 
- [2025/06/03] ðŸ‘‹ Upload paper and init project. [Read](https://arxiv.org/abs/****)

## ðŸŽ¥ Demo


## :pencil: To Do List
- [ ] Code and Config Release 
- [x] Technical Report


<!-- :hammer: Installation -->


<!-- ðŸŽ¯ Quick Start -->


## :notebook: Citation

```bibtex
@article{chen2025sparsevdit,
  title={Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers}, 
  author={Pengtao Chen and Xianfang Zeng and Maosen Zhao and Peng Ye and Mingzhu Shen and Wei Cheng and Gang Yu and Tao Chen},
  journal={arXiv preprint arxiv:},
  year={2025}
}
```

## :dizzy: Acknowledgments
We thank the following excellent open-source works: [Sparse-VideoGen](https://github.com/svg-project/Sparse-VideoGen), [MInference](https://github.com/microsoft/MInference), [PAB](https://github.com/NUS-HPC-AI-Lab/VideoSys), [CogVideoX](https://github.com/THUDM/CogVideo), [HunyuanVideo](https://github.com/Tencent-Hunyuan/HunyuanVideo), [Wan2.1](https://github.com/Wan-Video/Wan2.1).
